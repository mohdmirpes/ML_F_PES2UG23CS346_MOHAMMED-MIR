{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQVL4UhEUn7f"
   },
   "source": [
    "**Objective**: In this lab, you will implement and compare manual grid search with scikit-learn's built-in GridSearchCV for hyperparameter tuning. You'll work with multiple classification algorithms and combine them using voting classifiers.\n",
    "\n",
    "**Learning Goals**:\n",
    "- Understand hyperparameter tuning through grid search\n",
    "- Compare manual implementation with built-in functions\n",
    "- Learn to create and evaluate voting classifiers\n",
    "- Work with multiple real-world datasets\n",
    "- Visualize model performance using ROC curves and confusion matrices\n",
    "\n",
    "**Datasets Used**:\n",
    "1. Wine Quality - Predicting wine quality based on chemical properties\n",
    "2. HR Attrition - Predicting employee turnover\n",
    "3. Banknote Authentication - Detecting counterfeit banknotes\n",
    "4. QSAR Biodegradation - Predicting chemical biodegradability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhTLi8QnUxFG"
   },
   "source": [
    "\n",
    "## Part 1: Import Libraries and Setup\n",
    "\n",
    "First, let's import all the necessary libraries for our machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OwDmFPCRrMI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, roc_curve,\n",
    "                           confusion_matrix, ConfusionMatrixDisplay, classification_report)\n",
    "# Bypass SSL certificate verification for dataset downloads\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0WVjgxFUwIO"
   },
   "source": [
    "# Models and Parameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBkCgnvWRvcK"
   },
   "outputs": [],
   "source": [
    "# COMPLETED TODO: Define base models (Decision Tree, kNN, Logistic Regression)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# COMPLETED TODO: Define parameter grids\n",
    "# The parameter names must match the pipeline step names, e.g., 'classifier__max_depth'\n",
    "param_grid_dt = {\n",
    "    'feature_selection__k': [5, 10, 15, 'all'],\n",
    "    'classifier__max_depth': [3, 5, 7, 10, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'feature_selection__k': [5, 10, 15, 'all'],\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'feature_selection__k': [5, 10, 15, 'all'],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# COMPLETED TODO: Create a list of (classifier, param_grid, name) tuples\n",
    "classifiers_to_tune = [\n",
    "    (dt_classifier, param_grid_dt, \"Decision Tree\"),\n",
    "    (knn_classifier, param_grid_knn, \"k-Nearest Neighbors\"),\n",
    "    (lr_classifier, param_grid_lr, \"Logistic Regression\")\n",
    "]\n",
    "\n",
    "print(\"Models and parameter grids defined successfully!\")\n",
    "print(f\"Number of classifiers to tune: {len(classifiers_to_tune)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Ls-UHJWNU6"
   },
   "source": [
    "## Dataset Loading Functions\n",
    "We'll work with four different datasets to test our algorithms across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUtGII7gWX_a"
   },
   "source": [
    "### 3.1 Wine Quality Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHHtEi5GZjaU"
   },
   "outputs": [],
   "source": [
    "def load_wine_quality():\n",
    "    \"\"\"Load Wine Quality dataset\"\"\"\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "    try:\n",
    "        data = pd.read_csv(url, sep=';')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Wine Quality dataset: {e}\")\n",
    "        return None, None, None, None, \"Wine Quality (Failed)\"\n",
    "\n",
    "    # Create the binary target variable 'good_quality'\n",
    "    data['good_quality'] = (data['quality'] > 5).astype(int)\n",
    "    X = data.drop(['quality', 'good_quality'], axis=1)\n",
    "    y = data['good_quality']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Wine Quality dataset loaded and preprocessed successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"Wine Quality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COsVTdScWkV6"
   },
   "source": [
    "### 3.2 HR Attrition Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg6n6BKwZwGU"
   },
   "outputs": [],
   "source": [
    "def load_hr_attrition():\n",
    "    \"\"\"Load IBM HR Attrition dataset\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(\"data/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"HR Attrition dataset not found. Please place 'WA_Fn-UseC_-HR-Employee-Attrition.csv' inside a 'data/' folder.\")\n",
    "        return None, None, None, None, \"HR Attrition (Failed)\"\n",
    "\n",
    "    # Target: Attrition = Yes (1), No (0)\n",
    "    data['Attrition'] = (data['Attrition'] == 'Yes').astype(int)\n",
    "\n",
    "    # Drop ID-like column\n",
    "    X = data.drop(['EmployeeNumber', 'Attrition'], axis=1, errors='ignore')\n",
    "    y = data['Attrition']\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"IBM HR Attrition dataset loaded and preprocessed successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"HR Attrition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKtpzNthWoUu"
   },
   "source": [
    "### 3.3 Banknote Authentication Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx-CvEtlZ1HO"
   },
   "outputs": [],
   "source": [
    "def load_banknote():\n",
    "    \"\"\"Load Banknote Authentication dataset\"\"\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(url, header=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Banknote dataset: {e}\")\n",
    "        return None, None, None, None, \"Banknote (Failed)\"\n",
    "\n",
    "    # According to UCI: variance, skewness, curtosis, entropy, class (0=authentic, 1=fake)\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Banknote Authentication dataset loaded successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"Banknote Authentication\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvL5YtzRW8QT"
   },
   "source": [
    "\n",
    "### 3.4 QSAR Biodegradation Dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bec4DanzZ60c"
   },
   "outputs": [],
   "source": [
    "def load_qsar_biodegradation():\n",
    "    \"\"\"Load QSAR Biodegradation dataset\"\"\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\"\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(url, sep=';', header=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading QSAR dataset: {e}\")\n",
    "        return None, None, None, None, \"QSAR (Failed)\"\n",
    "\n",
    "    # Last column is target (RB = ready biodegradable, NRB = not)\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = (data.iloc[:, -1] == 'RB').astype(int)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"QSAR Biodegradation dataset loaded successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"QSAR Biodegradation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ovvQpiIXN7O"
   },
   "source": [
    "## Part 4: Manual Grid Search Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GJmCWupaGE8"
   },
   "outputs": [],
   "source": [
    "def run_manual_grid_search(X_train, y_train, dataset_name):\n",
    "    \"\"\"Run manual grid search and return best estimators\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING MANUAL GRID SEARCH FOR {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best_estimators = {}\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Adjust parameter grids based on dataset size\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    for classifier_instance, param_grid, name in classifiers_to_tune:\n",
    "        print(f\"--- Manual Grid Search for {name} ---\")\n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "\n",
    "        # COMPLETED TODO: Implement manual grid search for hyperparameter tuning.\n",
    "        # Steps implemented:\n",
    "        # 1. Adjust the feature selection parameter grid to ensure 'k' does not exceed the number of features.\n",
    "        adjusted_param_grid = param_grid.copy()\n",
    "        if 'feature_selection__k' in adjusted_param_grid:\n",
    "            k_values = []\n",
    "            for k in adjusted_param_grid['feature_selection__k']:\n",
    "                if k == 'all':\n",
    "                    k_values.append(n_features)\n",
    "                elif isinstance(k, int) and k <= n_features:\n",
    "                    k_values.append(k)\n",
    "            adjusted_param_grid['feature_selection__k'] = k_values\n",
    "\n",
    "        # 2. Generate all combinations of hyperparameters from the adjusted parameter grid.\n",
    "        param_names = list(adjusted_param_grid.keys())\n",
    "        param_values = list(adjusted_param_grid.values())\n",
    "        \n",
    "        total_combinations = 1\n",
    "        for values in param_values:\n",
    "            total_combinations *= len(values)\n",
    "        \n",
    "        print(f\"Testing {total_combinations} parameter combinations...\")\n",
    "\n",
    "        # 3. For each parameter combination:\n",
    "        for i, param_combination in enumerate(itertools.product(*param_values)):\n",
    "            # Create parameter dictionary for current combination\n",
    "            current_params = dict(zip(param_names, param_combination))\n",
    "            \n",
    "            # a. Perform cross-validation (5-fold StratifiedKFold).\n",
    "            cv_scores = []\n",
    "            \n",
    "            # b. For each fold:\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "                # i. Split the training data into training and validation sets.\n",
    "                X_train_fold = X_train.iloc[train_idx]\n",
    "                X_val_fold = X_train.iloc[val_idx]\n",
    "                y_train_fold = y_train.iloc[train_idx]\n",
    "                y_val_fold = y_train.iloc[val_idx]\n",
    "                \n",
    "                # ii. Build a pipeline with scaling, feature selection, and the classifier.\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('feature_selection', SelectKBest(f_classif)),\n",
    "                    ('classifier', classifier_instance.__class__(**classifier_instance.get_params()))\n",
    "                ])\n",
    "                \n",
    "                # iii. Set the pipeline parameters for the current combination.\n",
    "                pipeline.set_params(**current_params)\n",
    "                \n",
    "                # iv. Fit the pipeline on the training fold.\n",
    "                pipeline.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # v. Predict probabilities on the validation fold.\n",
    "                y_pred_proba = pipeline.predict_proba(X_val_fold)[:, 1]\n",
    "                \n",
    "                # vi. Compute the AUC score for the fold.\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "                    cv_scores.append(auc_score)\n",
    "                except ValueError:\n",
    "                    # In case of issues with AUC calculation\n",
    "                    cv_scores.append(0.5)\n",
    "            \n",
    "            # c. Compute the mean AUC across all folds for the parameter combination.\n",
    "            mean_auc = np.mean(cv_scores)\n",
    "            \n",
    "            # d. Track and print the best parameter combination and its mean AUC.\n",
    "            if mean_auc > best_score:\n",
    "                best_score = mean_auc\n",
    "                best_params = current_params\n",
    "            \n",
    "            # Print progress for every 10th combination\n",
    "            if (i + 1) % 10 == 0 or (i + 1) == total_combinations:\n",
    "                print(f\"  Tested {i+1}/{total_combinations} combinations. Current best AUC: {best_score:.4f}\")\n",
    "\n",
    "        # Create the final pipeline for this classifier\n",
    "        print(\"-\" * 90)\n",
    "        print(f\"Best parameters for {name}: {best_params}\")\n",
    "        print(f\"Best cross-validation AUC: {best_score:.4f}\")\n",
    "\n",
    "        final_pipeline = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('feature_selection', SelectKBest(f_classif)),\n",
    "            ('classifier', classifier_instance.__class__(**classifier_instance.get_params()))\n",
    "        ])\n",
    "\n",
    "        # Set the best parameters found\n",
    "        final_pipeline.set_params(**best_params)\n",
    "\n",
    "        # Fit the final pipeline on the full training data\n",
    "        final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Store the fully trained best pipeline\n",
    "        best_estimators[name] = final_pipeline\n",
    "\n",
    "    return best_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRUAXSXaXUJG"
   },
   "source": [
    "**Understanding the Manual Implementation**:\n",
    "- **Nested Cross-Validation**: For each parameter combination, we perform 5-fold CV\n",
    "- **Pipeline Integration**: Each step (scaling, feature selection, classification) is properly chained\n",
    "- **AUC Scoring**: We use Area Under the ROC Curve as our optimization metric\n",
    "- **Best Model Selection**: The combination with highest mean AUC across folds is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-4A_DGpXotU"
   },
   "source": [
    "## Part 5: Built-in Grid Search Implementation\n",
    "\n",
    "Now let's compare our manual implementation with scikit-learn's GridSearchCV.\n",
    "\n",
    "\n",
    "\n",
    "**Advantages of Built-in GridSearchCV**:\n",
    "- **Parallel Processing**: Uses `n_jobs=-1` for faster computation\n",
    "- **Cleaner Code**: Less verbose than manual implementation\n",
    "- **Built-in Features**: Automatic best model selection and scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edhV1LPEaOdb"
   },
   "outputs": [],
   "source": [
    "def run_builtin_grid_search(X_train, y_train, dataset_name):\n",
    "    \"\"\"Run built-in grid search and return best estimators\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING BUILT-IN GRID SEARCH FOR {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    results_builtin = {}\n",
    "\n",
    "    # Adjust parameter grids based on dataset size\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    for classifier_instance, param_grid, name in classifiers_to_tune:\n",
    "        print(f\"\\n--- GridSearchCV for {name} ---\")\n",
    "\n",
    "        # COMPLETED TODO: Implement built-in grid search for each classifier:\n",
    "        # - Adjust feature selection parameter grid based on dataset size (n_features)\n",
    "        adjusted_param_grid = param_grid.copy()\n",
    "        if 'feature_selection__k' in adjusted_param_grid:\n",
    "            k_values = []\n",
    "            for k in adjusted_param_grid['feature_selection__k']:\n",
    "                if k == 'all':\n",
    "                    k_values.append(n_features)\n",
    "                elif isinstance(k, int) and k <= n_features:\n",
    "                    k_values.append(k)\n",
    "            adjusted_param_grid['feature_selection__k'] = k_values\n",
    "\n",
    "        # - Create a pipeline with StandardScaler, SelectKBest(f_classif), and the classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('feature_selection', SelectKBest(f_classif)),\n",
    "            ('classifier', classifier_instance.__class__(**classifier_instance.get_params()))\n",
    "        ])\n",
    "        \n",
    "        # - Set up StratifiedKFold cross-validation\n",
    "        cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # - Run GridSearchCV with the pipeline and adjusted param grid\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=adjusted_param_grid,\n",
    "            cv=cv_splitter,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,  # Use all available cores\n",
    "            verbose=1   # Show progress\n",
    "        )\n",
    "\n",
    "        # - Fit grid search on training data and collect best estimator/results\n",
    "        print(f\"Fitting GridSearchCV for {name}...\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Save results\n",
    "        results_builtin[name] = {\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'best_score (CV)': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        print(f\"Best params for {name}: {results_builtin[name]['best_params']}\")\n",
    "        print(f\"Best CV score: {results_builtin[name]['best_score (CV)']:.4f}\")\n",
    "\n",
    "    return results_builtin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk7sL-SRXlow"
   },
   "source": [
    "## Part 6: Model Evaluation and Voting Classifiers\n",
    "\n",
    "This function evaluates individual models and creates voting classifiers to combine their predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLlH3BDoabcE"
   },
   "outputs": [],
   "source": [
    "def evaluate_models(X_test, y_test, best_estimators, dataset_name, method_name=\"Manual\"):\n",
    "    \"\"\"Evaluate models and create visualizations\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING {method_name.upper()} MODELS FOR {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Individual model evaluation\n",
    "    print(f\"\\n--- Individual Model Performance ---\")\n",
    "    for name, model in best_estimators.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(f\"  Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"  Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"  F1-Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    # Voting Classifier\n",
    "    print(f\"\\n--- {method_name} Voting Classifier ---\")\n",
    "\n",
    "    if method_name == \"Manual\":\n",
    "        # Manual voting implementation\n",
    "        y_pred_votes = []\n",
    "        y_pred_proba_avg = []\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            votes = []\n",
    "            probas = []\n",
    "\n",
    "            for name, model in best_estimators.items():\n",
    "                pred = model.predict(X_test.iloc[[i]])[0]\n",
    "                proba = model.predict_proba(X_test.iloc[[i]])[0, 1]\n",
    "                votes.append(pred)\n",
    "                probas.append(proba)\n",
    "\n",
    "            majority_vote = 1 if np.mean(votes) > 0.5 else 0\n",
    "            avg_proba = np.mean(probas)\n",
    "\n",
    "            y_pred_votes.append(majority_vote)\n",
    "            y_pred_proba_avg.append(avg_proba)\n",
    "\n",
    "        y_pred_votes = np.array(y_pred_votes)\n",
    "        y_pred_proba_avg = np.array(y_pred_proba_avg)\n",
    "\n",
    "    else:  # Built-in\n",
    "        # Create VotingClassifier\n",
    "        estimators = [(name, model) for name, model in best_estimators.items()]\n",
    "        voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "        voting_clf.fit(X_train, y_train)  # Note: This assumes X_train, y_train are in scope\n",
    "\n",
    "        y_pred_votes = voting_clf.predict(X_test)\n",
    "        y_pred_proba_avg = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute voting metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_votes)\n",
    "    precision = precision_score(y_test, y_pred_votes, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_votes, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_votes, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba_avg)\n",
    "\n",
    "    print(f\"Voting Classifier Performance:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}, Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Visualizations\n",
    "    # ROC Curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, model in best_estimators.items():\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "    # Add voting classifier to ROC\n",
    "    fpr_vote, tpr_vote, _ = roc_curve(y_test, y_pred_proba_avg)\n",
    "    plt.plot(fpr_vote, tpr_vote, label=f'Voting (AUC = {auc:.3f})', linewidth=3, linestyle='--')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {dataset_name} ({method_name})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Confusion Matrix for Voting Classifier\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cm = confusion_matrix(y_test, y_pred_votes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=plt.gca(), cmap=\"Blues\")\n",
    "    plt.title(f'Voting Classifier - {dataset_name} ({method_name})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred_votes, y_pred_proba_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwzQEU5pZMDI"
   },
   "source": [
    "## Part 7: Complete Pipeline Function\n",
    "\n",
    "This function orchestrates the entire experiment for each dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGPXOfWaaf_n"
   },
   "outputs": [],
   "source": [
    "def run_complete_pipeline(dataset_loader, dataset_name):\n",
    "    \"\"\"Run complete pipeline for a dataset\"\"\"\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"PROCESSING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test, actual_name = dataset_loader()\n",
    "    if X_train is None:\n",
    "        print(f\"Skipping {dataset_name} due to loading error.\")\n",
    "        return\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Part 1: Manual Implementation\n",
    "    manual_estimators = run_manual_grid_search(X_train, y_train, actual_name)\n",
    "    manual_votes, manual_proba = evaluate_models(X_test, y_test, manual_estimators, actual_name, \"Manual\")\n",
    "\n",
    "    # Part 2: Built-in Implementation\n",
    "    builtin_results = run_builtin_grid_search(X_train, y_train, actual_name)\n",
    "    builtin_estimators = {name: results['best_estimator']\n",
    "                         for name, results in builtin_results.items()}\n",
    "    builtin_votes, builtin_proba = evaluate_models(X_test, y_test, builtin_estimators, actual_name, \"Built-in\")\n",
    "\n",
    "    print(f\"\\nCompleted processing for {actual_name}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luYsJ4GtZbz7"
   },
   "source": [
    "## Part 8: Execute the Complete Lab\n",
    "\n",
    "Now let's run our pipeline on all four datasets!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwgQCoI1ZcSa"
   },
   "outputs": [],
   "source": [
    "# --- Run Pipeline for All Datasets ---\n",
    "datasets = [\n",
    "    (load_wine_quality, \"Wine Quality\"),\n",
    "    (load_hr_attrition, \"HR Attrition\"),\n",
    "    (load_banknote, \"Banknote Authentication\"),\n",
    "    (load_qsar_biodegradation, \"QSAR Biodegradation\")\n",
    "]\n",
    "\n",
    "# Run for each dataset\n",
    "for dataset_loader, dataset_name in datasets:\n",
    "    try:\n",
    "        run_complete_pipeline(dataset_loader, dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DATASETS PROCESSED!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
